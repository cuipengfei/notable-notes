---
tags: [geek-time]
title: 程序员的AI开发第一课
created: '2024-12-25T14:55:24.715Z'
modified: '2024-12-25T14:55:24.715Z'
---

# 程序员的AI开发第一课

## 开篇词 | 普通程序员如何迈进AI时代？

### 尽量地把重点放在那些可能长期有效的东西上

### 大模型的微调，同 RAG 类似，微调是另外一种帮助大模型扩展知识面的技术，不同的是，RAG 是把知识补充到提示词里，而微调则是在模型上动了手脚。RAG 和微调各有千秋，RAG 成本更低，微调效果更好（如果做得好的话），AI 应用常常根据自己的需要选择其中一个。不过，某些项目会选择将二者都包含其中。

### 一些工程实践帮你提前规避一些问题，我会谈到如何实现记忆（让大模型更了解用户）和缓存（节省使用大模型的成本），以及统一接入（使用统一接口调用不同的模型）。

### Agent 则可以扩展模型能力的边界，比如，让它去做超越语言的事情。

### RAG（Retrieval-augmented generation，检索增强生成）帮助模型扩展了自己的知识面，了解更多它在训练时还不知道的东西

### 作为应用的编写者，我们很擅长把规则编写进代码，但什么规则可以由模型判定，则是我们作为普通程序员需要补充的知识。

### 如果说传统的应用开发是程序员把执行规则一条条写在代码里，那么基于大模型的 AI 应用开发最大的差异就是，有些执行规则是由模型替我们判定的。

### 当真正接触了 AI 应用开发，你就会发现，我们完全可以把别人构建好的应用当做基础设施来用，站在这个角度上看，这同我们使用 API 构建应用没有本质区别。不过，与我们简单地调用 API 传入参数不同，基于大模型的 AI 应用开发需要我们有不同的编程模式，而这正是我们要在这门课里学习的。

### 虽然 AI 时代是构建在许多顶尖的 AI 算法基础之上，但是，并非所有 AI 相关的工作都需要极强的 AI 算法功底。

### AI 时代，程序员依然有用

## 01｜用户视角：你应该知道的LLM基础知识

### 大语言模型（Large Language Model，LLM）。
### LLM 这个名字告诉我们三件事：
### 它是模型，是 AI 能力的核心。
### 它是语言模型，其核心能力在于语言能力。
### 它是大语言模型，与传统模型相比，它最大的特点就是“大”。

## 02｜技术视角：你应该知道的LLM基础知识

### 经过这个处理之后，一个硕大的向量就会压缩成一个固定大小的向量。比如，在 GPT 3 中，每个 Token 都由 768 个数字组成的向量表示。之后，再把压缩过的向量作为输入传给大模型，用来生成最终的结果，这时才轮到我们听说过的神经网络、自注意力机制这些东西登场

### 在一个有规模的语料库中，Token 数量会非常多

### One-Hot 编码就是将离散的分类值转换为二进制向量

### 在 GPT 3 中，每个 Token 都由 768 个数字组成的向量表示

### 典型的稀疏存储

### 在大模型内部处理的并不是字符串，而是向量。之所以要将字符串转换为向量，简单理解，就是现在大部分的 AI 算法只支持向量。
### 你可能会好奇，为啥不支持字符串呢？因为字符串只是这些 AI 算法的一种输入，我们还可以输入图片、输入视频，甚至各种各样的信息，只要把这些输入都转换成向量，AI 算法都可以轻松地驾驭。由此，你可以知道，交给 AI 算法处理的前提就是把各种信息转换成向量。

### 另一个重要概念：Embedding

### 我们会从一个带有概率的词列表中进行选择。接下来，我们需要确定选择哪个词添加到我们的内容中。
### 有人认为应该选择“概率最高”的词。但是，如果我们总是选择排名最高的词，通常会得到一篇非常“平淡”的文章，完全显示不出任何“创造力”。这就好比你问别人一个问题，他每次都给你同样的回答，你一定会觉得这个人非常的无趣。所以，在选择的时候，我们可以有一些随机性，选择排名较低的词，这样我们就可以得到一篇“更有趣”的文章。
### 有了随机性，也意味对于同样的提示词，我们每次得到的内容可能会不同。为了达到这样的效果，我们就引入了一个表示随机性强弱的概念：温度（Temperature）。这是大模型编程中另一个重要的概念。
### 很显然，温度这个概念借鉴自物理学，不过，它和物理学之间并没有实际的联系。站在理解这个概念的角度，你可以认为它是表示大模型活跃程度的一个参数，通过调节这个参数，大模型变得更加活跃，或是更加死板。
### 在大模型编程中，温度是一个非常重要的概念，温度设定的情况很大程度上决定了大模型给出怎样的表现。在 OpenAI 的 API 中，这个参数越小，表示确定性越强，越大，表示随机性越强，简单理解就是，温度越高越活跃。

### Token 的概念在大模型编程中是非常重要的，现在各大厂商的竞争中，有一个很重要的指标就是上下文窗口（Context Window）的大小。这里的上下文窗口，指的就是大模型可以处理 Token 数量，上下文越大，能处理的 Token 越多。能处理的 Token 越多，大模型对信息的理解就越充分，生成的内容就越接近我们需要的结果。所以，现在各个大模型比拼的其中一项就是上下文窗口的大小。

### Token 就是我们理解大模型编程的第一个重要概念。这个 Token 可能是我们传统理解的一个完整的单词，可能是一个单词组合，甚至可能是单词的一部分

### What Is ChatGPT Doing … and Why Does It Work?

### 大模型的工作很简单，一次添加一个词。

## 04｜提示工程：更好地释放LLM的能力

### 几个基本的提示技术：
### 零样本提示：用于通用的任务。
### 少样本提示：用于特定的简单任务。
### 思维链提示：引入推理过程，可以与零样本提示和少样本提示结合。
### ReAct 框架：将大模型推理和一些行动能力结合起来，超越大模型自身的限制。

### 提示工程并不是在 GPT 流行之后才诞生的技术。提示工程最早是诞生于自然语言处理（Natural Language Processing，简称 NLP），人们发现，在任务处理过程中，如果给予 AI 适当的引导，它能更准确地理解我们的意图，响应我们的指令。

## 05｜OpenAI API：LLM编程的事实标准（上）

### user，终端用户标识，它是我们作为开发者提供给 OpenAI 的，主要就是用作监控和检测 API 的滥用，监控粒度就到了个体上。

### max_completion_tokens，它表示生成应答的最大 token 数

### 角色主要是系统（system）和用户（user），你可以理解开发者设置的就是系统，用户的问题就是用户。有时，我们还会把大模型生成的消息添加进去，比如，聊天历史，它的角色是助理（assistant）。

### 大模型编程接口的路径是 /v1/chat/completions 。其中，v1 是版本号，这个接口起的名字叫聊天补全。补全，这个名字刚好也呼应了我们前面讲的 GPT 的工作原理。

## 06｜OpenAI API：LLM编程的事实标准（下）

### 除了聊天补全接口，还有一个 Embeddings API 在开发大模型应用中是非常常用的

### 如果严格遵守 SSE 程序库处理 OpenAI API ，就可能会遇到无法 POST 请求 SSE 的情况。

### 如果采用 WebSocket 的话，服务端就需要维护连接，像 OpenAI 这样的服务体量，维护连接就会造成很大的服务器压力，而且，在生成内容场景下，也没有向服务端进一步发送内容，WebSocket 的双向通信在这里也是多余的。

### OpenAI 之所以选择 SSE，而非 WebSocket，是因为 SSE 的技术特点刚好可以契合流式应答的需求：客户端与大模型的交互是一次性的，每产生一个 token，服务端就可以给客户端推送一次，当生成内容结束时，断掉连接，无需考虑客户端的存活情况。

### 流式应答可以采用什么技术实现呢？如果你做过服务端的开发，像这种涉及到服务端主动向客户端推送内容，我们可能会选择 Websocket，但 OpenAI 在这个问题上却选择了 SSE 这种技术。

## 07｜LangChain：一个AI应用开发生态

### LangChain 不只是一个框架，而是一个生态系统。

### LangChain 最重要的价值是提供了一些开发应用所需的基础抽象和 LangChain 表达式语言

## 08｜LangChain：核心抽象

### LangChain 提供的最核心的三个抽象：
### ChatModel：整个框架的核心，根据输入的内容生成输出。
### PromptTemplate：负责处理输入，有效拆分了开发者提示词和用户提示词。
### OutputParser：负责处理输出，许多输出解析器里包含了格式指令。
### 如果今天的内容你只能记住一件事，那请记住：ChatModel 是核心，PromptTemplate 处理输入，OutputParser 处理输出。

### LangChain 中最核心的三个抽象：ChatModel、PromptTemplate 和 OutputParser

### PromptTemplate 处理的是输入，与之对应的是 OutputParser，从名字就可以看出，它是负责处理输出的

### 许多大模型应用本质上就是开发者预置好提示词，把它和用户的提示词结合在一起发给大模型，以便达到更好的效果。

### 所有的 LangChain 应用代码核心就是构建一条链。在这里，单独的一个模型也是一条链，只不过这条链上只有一个组件，它就是 ChatModel。前面的两个例子演示的调用方法其实也是链的调用方法。在后面的例子里，我会把重点放在链的组装上，返回结果如何处理在这两个例子里已经做了相应的演示。

### LangChain 中既有 LLM，也有 ChatModel？
### 它俩的关系其实类似于之前我们说的补全接口和聊天补全接口的关系，LLM 对应的是文本到文本的生成，而 ChatModel 则是对应着由 ChatGPT 带来的聊天模式。大部分情况下，推荐使用 ChatModel，即便对于非聊天应用也是如此，如同聊天补全接口几乎可以替代补全接口，ChatModel 几乎可以完全替代 LLM

## 09｜从零实现一个角色扮演的聊天机器人

### 实现一个聊天机器人的关键点就是管理好聊天历史。

### 在实现聊天机器人的过程中，还有一个很现实的问题，我们需要处理一下。如果不加任何限制，所有的聊天历史都会附加到新的会话中，随着聊天的进行，聊天历史很快就会超过大模型的上下文窗口大小。一种典型处理办法是，对聊天历史进行限制。
### LangChain 提供了一个 trim_messages 用来控制消息的规模，它提供了很多控制消息规模的参数：

### 为了支持聊天历史，LangChain 引入了一个抽象叫 ChatMessageHistory。为了简单，我们这里使用了 InMemoryChatMessageHistory，也就是在内存存放的聊天历史。有了聊天历史，就需要把聊天历史和模型结合起来，这就是 RunnableWithMessageHistory 所起的作用，它就是一个把聊天历史和链封装到一起的一个类。
### 这里的 Runnable 是一个接口，它表示一个工作单元。我们前面说过，组成链是由一个一个的组件组成的。严格地说，这些组件都实现了 Runnable 接口，甚至链本身也实现了 Runnable 接口，我们之前讨论的 invoke、stream 等接口都是定义在 Runnable 里，可以说，Runnable 是真正的基础类型，LCEL 之所以能够以声明式的方式起作用，Runnable 接口是关键。

### OpenAI API，它是一个 HTTP 请求。我们知道，HTTP 请求最大的特点是无状态，也就是说，服务端不会保留任何会话信息。对于每次都完成一个独立的任务，无状态是没有任何问题的。但对聊天机器人来说，就会出现对之前会话一无所知的情况。这显然无法满足我们对一个聊天机器人的基本预期，我们需要解决这个问题。

## 10｜RAG：让大模型知道更多东西

### RAG 是为了让大模型知道更多的东西。

### RAG 是 Retrieval-Augmented Generation 的缩写，也就是检索增强生成的意思。它主要是为了解决大模型本身知识匮乏的问题。RAG 应用的主要流程包括索引、检索和生成。

### 我们前面讨论的流程前提条件是把原始信息转换成了向量，但这本质上还是基于文本的，更适合回答一些事实性问题。它无法理解更复杂的关系，比如，我的朋友里谁在 AI 领域里工作。所以，有人提出了基于知识图谱的 RAG，知识图谱是一种结构化的语义知识库，特别适合找出信息之间的关联。
### 由此你可以看到，想要打造一个好的 RAG 应用并不是很容易的一件事，但在一些技术框架支持下，上手编写一个 RAG 应用却不是什么难事

### 到这里，我们讲的都是怎样使用数据，也就是检索的过程。其实，还有一个关键的问题没有解决，这些数据从何而来，怎么跑到向量数据库里去的。这就是 RAG 另外一个重要的过程：索引（Indexing）。

### 向量数据库与传统数据库有很大的差别，在使用方式上，传统数据库搜索信息倾向于精确匹配，而向量数据库的匹配则是语义上的接近。

### 在 RAG 系统中，我们要把数据存放到哪里呢？我们需要一个数据库，只不过，我们需要的既不是 Oracle、MySQL 这样的关系数据库，也不是 MongoDB、Redis 这样的 NoSQL 数据库。因为我们后续处理的都是向量，所以，我们需要的是向量数据库。

### OpenAI 就提供了一个专门负责将文本转换成向量的 API——Embeddings。我们可以根据需要，选择自己部署模型，或是选择别人提供的服务。不同的 Embedding 模型之间的差异主要取决于训练样本，比如有的模型会在中文处理上表现得比较好。

### 许多 AI 算法处理的都不是文字，而是向量。采用向量的方案，“语义”的匹配程度就转换成了向量之间的相似程度。计算向量相似度的算法有很多，比如，余弦相似度、内积、欧氏距离等等。
### 有了向量，当用户提出问题时，处理过程就变成了将问题转换为向量，然后计算向量之间的距离，找到与问题向量最接近的文档向量，从而实现“语义”的匹配。

### 在业务系统开发中，我们经常做的文本匹配是用 SQL 语句的 like。但是，这种匹配对于大模型应用而言，就显得非常单薄了，因为它只能进行“字面”意义上的匹配，无法进行“语义”层面的匹配。如果想进行“语义”的匹配该怎么做呢？这就轮到向量登场了。

### 对比普通的聊天机器人，差别就是在这个过程中，要去到相关资料中进行查询，再将查询得到的相关信息和用户问题拼装成完整的提示词发给大模型。

### 用户发起请求
### 根据用户的问题在相关资料中进行查询
### 获取到资料中的相关内容
### 组成完整的提示词发给大模型
### 将大模型的回复发给用户

### 怎么让大模型了解我们的业务呢？最简单的方式就是每次请求时，把业务相关的内容放在提示词里告诉大模型。
### 问题是，业务相关的内容从何而来？这就是我们要在本地检索的内容。换言之，所谓检索增强生成，就是我们在本地检索到相关的内容，把它增强到提示词里，然后再去做内容生成。

### 如果我打算为自己的业务开发一个聊天机器人，也就是说，让聊天机器人知道我的业务，该怎么办呢？抛开训练一个属于自己的大模型这种成本高昂的办法，常见的解决方案有两种：
### 模型微调：使用业务信息对已经训练好的模型进行微调。
### RAG：在上下文中带有业务信息，让大模型据此进行整合。
### 相比于模型微调，RAG 的方案成本要低一些，而且更加灵活，实现起来也更简单一些

### 虽然我们说大模型的特点之一是知识丰富，但这里的知识仅限于通用的知识，也就是网上能够很容易找到的知识。对于一些特定的知识，比如你所在业务领域的知识，它就一无所知了。个中缘由，不言而喻，大模型训练时，根本不可能拿到你们公司的数据。

## 11｜自己动手实现一个RAG应用

### 在 LangChain 代码里， | 运算符被用作不同组件之间的连接，其实现的关键就是大部分组件都实现了 Runnable 接口，在这个接口里实现了 __or__ 和 __ror__。__or__ 表示这个对象出现在| 左边时的处理，相应的 __ror__ 表示这个对象出现在右边时的处理。
### Python 在处理 a | b 这个表达式时，它会先尝试找 a 的 __or__，如果找不到，它会尝试找 b 的 __ror__。所以，在 context 的处理中， 来自标准库的 itemgetter 虽然没有实现
### __or__，但 retriever 因为实现了 Runnable 接口，所以，它也实现了 __ror__。所以，这段代码才能组装出我们所需的链。

### 为什么不直接使用向量数据库呢？因为 Retriever 并不只有向量数据库一种实现，比如，WikipediaRetriever 可以从 Wikipedia 上进行搜索。所以，一个 Retriever 接口就把具体的实现隔离开来。

### 这段代码引入了一个新的概念：Retriever。从名字不难看出，它就是充当 RAG 中的 R。Retriever 的核心能力就是根据文本查询出对应的文档（Document）。

### LangChain 支持了很多的向量数据库，它们都有一个统一的接口：VectorStore

### 在 LangChain 中，有一个很重要的概念叫文档（Document），它包括文档的内容（page_content）以及相关的元数据（metadata）。所有原始信息都是文档，索引信息的第一步就是把这些文档加载进来，这就是 DocumentLoader 的作用

### 我们知道 RAG 有两个核心的过程，一个是把信息存放起来的索引过程，一个是利用找到相关信息生成内容的检索生成过程。所以，我们这个 RAG 应用也要分成两个部分：索引和检索生成。

## 12｜Agent：从聊天到工作

### Agent 是一个软件系统，大模型给了它一个好“大脑”。

### 规划组件的能力是需要智能完成的，这个部分要归属于大脑，在实现中，我们可以让大模型来做这部分工作。在记忆组件中，短期记忆可以用聊天历史的方式解决，而长期记忆，我们可以存放到向量数据库中，采用类似 RAG 的方式解决。工具组件主要是与不同的内容集成，这个部分是程序员最熟悉的部分，属于常规的编码。

### 一些重要的组件：
### 规划（Planning），它负责将大目标分解成小的子目标，也可以对已有行为进行反思和自我改善。
### 记忆（Memory），包括短期记忆和长期记忆，短期记忆提供上下文内的学习，长期记忆则提供长时间保留和回忆信息的能力。
### 工具（Tools），通过调用外部 API 获取外部信息（作为感知器），执行外部动作（作为执行器）。

### 虽然很多人都在把大模型当作一个更好的聊天机器人在用，但实际上，大模型还有一个很强的能力，就是推理能力。所以，有人开始把大模型视为一个强大的通用问题解决器（general problem solver）。

### 传统的软件系统中，所有处理规则都是我们硬编码在其中的，是固定不变的，而在人工智能领域，这个“大脑”是具备灵活性的，它可以自行推断出下一步该做什么。
### 我们今天讨论的 Agent 是一种能够自主感知周围环境、做出决策、采取行动达成特定目标的系统。“自主”是 Agent 与传统软件系统之间的差异。

### 智能体通过传感器从外界感知环境，并将接收到的信息交给中央的“大脑”处理，然后，“大脑”做出决策，让执行器执行相应的动作，对环境产生影响。
### 根据书里的定义，任何通过传感器（sensor）感知环境（environment），并通过执行器（actuator）作用于该环境的事物都可以视为智能体（agent）

## 14｜用LangChain实现一个Agent

### 编写基于 LangChain 的 Agent，关键点是学会工具。

### 与之前几个基于 LangChain 的应用最大的不同在于，我们这个 Agent 的实现并没有组装成一个链。正如我们前面所说，Agent 的核心是一个循环，这其实是一个流程，而之前的应用从头到尾都是一个“链”式过程。所以，这里用到了 AgentExecutor。
### 即便不看它的实现，你应该也能知道，其核心实现就是一个循环：判断是不是该结束，不是的话，继续下一步，向大模型发送消息，是的话，跳出循环。

### Agent 在执行过程中，会获取工具的信息，传给大模型。这些信息主要就是一个工具的名称、描述和参数，这样大模型就知道该在什么情况下怎样调用这个工具了。@tool 可以提取函数名变成工具名，提取参数变成工具的参数，还有一点就是，它可以提取函数的 Docstring 作为工具的描述。这样一来，calculate 就从一个普通的函数变成了一个工具。

### @tool 是一个装饰器，它让我们把一个函数变成了一个工具（tool）。
### 工具在 LangChain 里是一个重要的概念，它和我们说的 Agent 系统架构中的工具概念是可以对应上的，工具主要负责执行查询，或是完成一个一个的动作。

## 15｜长期记忆：让大模型更了解你

### 拥有长期记忆的大模型应用可以更好地服务用户。

### 说了这么多 mem0 的优点，如果你真的选型时考虑它，也需要知道它的一些问题。作为一个起步时间不长的项目，它尚在剧烈的开发过程之中，变动会比较大，比如，在 1.1 版本中，mem0 引入了对图（Graph）的支持，发掘事物之间的关系。目前的 mem0 实现在每次添加信息时，都会调用大模型，这也就意味着成本的增加，这也是我们在选型时必须要考虑的。

### 理解了 mem0 是怎样工作的，你会发现，有了 mem0 实现的长期记忆，我们似乎就不再需要短期记忆了。因为我们会在拼装消息时，把相关上下文中从长期记忆中找出来。

### 但为什么还要配置大模型呢？因为 mem0 并不是把数据直接存到向量数据库里的。调用 add 时，mem0 会先把内容发送给大模型，让大模型从内容中提取出一些事实（fact），真正存放到向量数据库里的实际上是这些事实。

### 如果查看 mem0 的文档，你会发现它的 API 相当简单，无非是常见的增删改查。如果不是知道它的作用，我们甚至以为自己看到的是一个数据库的接口。这就是这个 API 设计好的地方：我们把长期记忆看作一个数据库，对长期记忆的处理相当于对数据库的访问，而复杂的细节隐藏在了简洁的接口之下。所以，从理解的角度看，它对我们几乎没有什么负担。

### 常见的一个思路是，把需要记忆的内容存放到向量数据库中，采用类似于 RAG 的方案，在生成的时候，先到向量数据库中进行索引，把索引到内容放到提示词里面。当然，在具体的实现里，什么样的内容是需要记忆的内容、怎样提取怎样的内容等等，都是需要解决的问题，更有甚者，有的实现还要实现深度的挖掘，找到不同事物之间的关系。

### 为什么长期记忆是一个问题？从本质上说，这是大模型上下文大小有限造成的问题。前面说过，几乎每个模型的上下文窗口都是有限的。如果上下文窗口是无限的，我们完全可以用短期记忆的解决方案，也就是把所有的聊天历史都发送给大模型，让大模型“记住”所有的东西。
### 不过，即便上下文可以很大，还有一个现实的问题，大模型是根据 Token 计费的，过大的上下文会让费用很高。再退一步，即便上下文窗口很大且不在乎计费，如何把过大的内容传输给大模型，也会是一个非常实际的工程问题。

### 能放到提示词的聊天历史是有限的，所以，它只能记住“近期”的事，这也是这种方案被称为短期记忆的原因。

### 大模型的 API 是无状态的，所以，大模型本质上是没有记忆的。大模型记忆的实现是通过在提示词中传递更多的内容实现的。

## 16｜缓存：节省使用大模型的成本

### 对大模型应用而言，缓存可以提升访问速度，降低访问成本。

### Redis 社区在向量的支持上也在继续努力，有一个项目 RedisVL（Redis Vector Library）就是把 Redis 当作向量数据库，有兴趣的话你可以了解一下。

### LangChain 社区确实已经有了实现 VectorStore 接口的 Redis，也就是说，我们完全可以用 Redis 替换之前讲过的向量存储。事实上，这里的语义缓存底层就是用了这个实现了 VectorStore 接口的 Redis。

### LangChain 支持的 Redis 缓存有精确缓存和语义缓存两种，RedisCache 对应的是精确缓存，RedisSemanticCache 对应的是语义缓存。

### 在大部分人眼中，Redis 应该属于精确匹配的缓存。Redis 这么多年也在不断地发展，有很多新功能不断地拓展出来，最典型的就是 Redis Stack，它就是在原本开源 Redis 基础上扩展了其它的一些能力。
### 比如，对 JSON 支持（RedisJSON），对全文搜索的支持（RediSearch），对时序数据的支持（RedisTimeSeries），对概率结构的支持（RedisBloom）。其中，支持全文搜索的 RediSearch 就可以用来实现基于语义的搜索。全文搜索，本质上也是语义搜索，而这个能力刚好就是我们在语义缓存中需要的。
### 你现在知道了，Redis 对于语义缓存的支持是基于 RediSearch 的。所以，要想使用语义缓存，我们需要使用安装了 RediSearch 的 Redis，一种方式是使用 Redis Stack

### 但大模型应用的特点就决定了精确缓存往往是失效的。因为大模型应用通常采用的是自然语言交互，以自然语言为提示词，就很难做到完全相同。像前面我展示的那个例子，实际上是我特意构建的，才能保证精确匹配。所以，语义匹配就成了更好的选择。

### 虽然 LangChain 提供了许多缓存实现，但本质上说，只有两类缓存——精确缓存和语义缓存。精确缓存，只是在提示词完全相同的情况下才能命中缓存，它和我们理解的传统缓存是一致的，我们前面用来演示的内存缓存就是精确缓存。

### 我们为什么要使用缓存呢？主要就是为了减少访问低速服务的次数，提高访问速度。大模型显然就是一个低速服务，甚至比普通的服务还要慢。

### 计算机科学中只有两大难题：缓存失效和命名。
### —— Phil Karlton

## 17｜集中接入：将大模型统一管理起来

### 笔记 · 2024年12月25日 15:19: 我们前面说过，LangChain 在一些场景下是不适用的，其中的一个原因就是它提供的一些抽象在某些情况下是失效的。有了大模型代理，LangChain 提供的模型抽象就显得没有必要了。

### 我直接用大模型不好吗？为什么还要在中间加上一层代理呢？
### 我在前面说过，集中接入是一种架构上的调整，顾名思义，我需要是一个服务，才会有架构调整的说法。如果只是像前面几讲，如果在本地就可以运行起来的一些程序，确实没有必要在中间加入一层。但在真实的项目中，我们往往是要构建一个服务，这时集中接入的价值就体现出来了。
### 之所以要有一个中间层，最直接的一个问题就是限流问题。大模型服务本身资源消耗很大，提供大模型服务的供应商为了保证尽可能多的用户享受到正常的服务，所以，它对单用户实施了限流。以 OpenAI API 为例，下面就是它的限流标准，其中 RPM 是 requests per minute（每分钟请求数），TPM 是 tokens per minute（每分钟 Token 数）。
### 如果我们是一个人或是规模比较小的服务，这个限流标准大概是够用的，但如果我们要对外提供服务，这个标准大概率是不够用的。解决这个问题最简单的办法就是多申请一些账号，形成一个号池，这样限流标准对我们来说就大幅度提高了，但随之而来的一个问题就是如何管理号池。
### 稍微仔细想一下，你就会发现，实现一个还不错的号池管理还是比较麻烦的。比如，按什么方式在不同的账号之间进行选择，怎样管理失效的账号等等。真的要实现好一个号池，就等于实现了一个完整的运维工具，可是，你的应用目标是做一个 AI 应用。与其自己实现这么一套完整的功能，还不如用已有的工具来完成这个目标。是的，已经有一些现成的工具可以完成这个目标。

## 18｜开源模型：如何使用Hugging Face上的模型

### Hugging Face 为我们提供了大量可以完成各种工作的模型。

### 今天的 Hugging Face 已经成为了任何一个团队发布模型的首选之地，我们要使用的各种模型几乎都能在 Hugging Face 上找到

## 19｜如何在项目中使用开源模型

### Ollama 是什么呢？它是一个可以在本地运行大模型的轻量级框架。同我们在本地运行代码访问大模型类似，它可以在本地启动一个服务，我们可以通过这个服务去使用相应的大模型。类似的东西也有不少，比如 vllm、gpt4all 等。

### 这个包封装了 Hugging Face 上的主要能力——模型和数据集。其中，不同的模型因为能力上的差异做了不同归结：属于大语言模型的，就归结到了 LangChain 模型上，而无法归结的，就以工具的形式提供。

### langchain-huggingface 是 Hugging Face 和 LangChain  共同维护的一个包，其目标是缩短将 Hugging Face 生态的新功能带给 LangChain 用户的时间

### LangChain 最基础的抽象——模型

### 两种常见的封装：使用 LangChain 和使用集中接入。

## 20｜模型微调：创建一个属于自己的大模型

### 模型微调的关键是准备好数据。

### 一个模型接入了 Ollama，它就能接入到 One-API 上，而接入了大模型代理，我们就可以在项目中访问它了。这样，我们就完成了一个完整的微调流程。

### 这其中最耗时耗力的，应该是准备数据了。首先，微调大模型需要多准备一些数据，数据量过少，可能看上去完全不起作用。毕竟大模型那么多参数，如果只改动几个，占比太低，就和没改动效果差不多。其次，数据也要精心挑选一下，还是那句话：垃圾进，垃圾出。所以，这个过程不是技术活，而是一个体力活。什么数据是好数据？最好由一个懂业务的人说了算。

### 在工程实践中，二者往往是结合使用的。模型微调不是时刻在进行，所以，一些团队的做法是，用 RAG 的方式提取新的业务数据，积累到一定阶段，用这些数据进行模型微调，把这些数据内置到模型中，再把新模型替换到业务系统中。这样一来，既保证数据的新鲜，又保证了基本的服务质量。

### 前面说过，RAG 和模型微调可以解决同样的问题，从本质上说，就是把核心业务数据放在提示词里，还是放在模型里。

### 大模型是构建于神经网络基础之上的，神经网络可以理解成一个一个的神经元构建的网络。训练模型，就是在调整神经元之间的连接方式。一次完整的训练就相当于把所有的神经元连接都调整一遍，这个计算规模相当之大，是我们无法承受的。
### 所谓微调，就是把一个训练好的模型中的一部分连接重新调整。因为只做了一部分的调整，所以，规模就要小得多，训练成本也就要小得多。

### 按照一些大公司的说法，一个千亿参数的大模型，训练一次的成本大约需要几百万美元

## 21｜如何面对不断更新的大模型

### 不变的是，大模型的 API 和基础能力。我们要利用这些能力构建自己的应用，而应用的核心是我们的业务，大模型是帮助我们提供更好的服务体验。
### 由于大模型基础能力的提高，我们可以在一个服务中使用多个不同的大模型，既可以节省成本，又可以找到最佳的表现。通过引入大模型路由，我们可以把其中的复杂度从应用中分离出来。
### 应对大模型的变，我们需要找到自己的核心关注点，是使用场景在增加，还是模型能力有提升，抑或是成本得到控制。只有找到自己的核心关注点，我们才不会在不断涌现的新模型中迷失。
### 如果今天的内容你只能记住一件事，那请记住，找到自己的关注点，让大模型为我所用。

### 笔记 · 2024年12月25日 17:06: 学过前面的课程，你可能会觉得这个结构有点类似于前面说过的大模型代理。二者不同的点在于，大模型代理是做一个独立的部分，它只提供标准的 OpenAI API，我们在调用时，需要指定模型，而大模型路由，则是要根据用途确定不同的大模型，请求方甚至都不用指定模型，因为我们到底使用哪个模型已经在路由内部配置好了。

### 在这个架构中，我们引入了一个 LLM Router，也就是大模型路由，它负责根据用途的差异，采用不同的模型。比如，聊天就用 Open AI 的服务，推理使用本地部署的开源模型。有了大模型路由，我们就可以把应用和使用不同模型隔离开来，以保证应用代码的稳定。

### 理解了大模型现在的水准，还可以帮助我们做出一些更好的架构决策。举个例子，我们之前好几次强调过 AI 应用的成本问题，一个重要的原因是，调用大模型服务，不管你的内容是什么，大模型服务都要按照同样的标准进行收费。
### 仔细分析一下，我们便不难发现，我们应用中，有一部分的工作属于简单的推理工作，比如我们介绍过的 ReAct，其中一个核心步骤就是确定调用哪个工具。如果我们搭建了一个本地的开源模型，这些简单的推理工作完全可以在本地完成，无需为此支付模型调用的费用，以此节省调用模型的成本。
### 一旦我们想清楚，不同的请求可以发给不同的模型，我们就可以在架构上做一个区分，一部分请求需要发给大模型服务获取更好的表现，一部分请求（比如简单推理）交给本地的模型，完成基本的操作。在这种思路下，整个的服务处理过程就会有多个不同的模型参与。既然可以引入一个额外的模型，我们完全可以再进一步，在处理过程中，引入多个模型，让不同的模型完成不同的工作。

### 开发一个 AI 应用，核心点并不是选择一个更好的大模型，而是自己的业务。在现阶段，再好的大模型也不能帮我们把业务理顺，再好的大模型也只能起到一个辅助的作用，不要过分高估大模型所能起到的作用。我们要做的是，用大模型改造我们的业务流程，让 AI 嵌入到我们的业务流程中去。

### 从 GPT 3.5 到 GPT 4，我们可以理解为是有了更多的数据，让大模型表现进了一步，而 GPT 4 到 GPT-o1，其实是一些工程上的进步，简单地理解，是把提示工程的技术内置到大模型里了。不光 Open AI 的大模型是这样，其它大模型也是如此。

## 结束语｜与AI共同成长

### 一个相对比较清晰的 Agent 框架是 crewAI。它可以通过配置文件配置出一个个可以协同工作的 Agent，并定义出它们协作的工作流。另外一个后起之秀是 OpenAI 的 Swarm。它也是比较清晰简单的。不过，它是在我写作这个专栏期间才诞生的，还有很长一段路要走。

### 如果你有兴趣了解它们的实现，我强烈推荐你去看看 Cline，因为它是开源的。其核心流程就是分析出要做什么，然后利用各种工具完成相应的工作，这里的工具就是各种对 IDE 的操作。没错，这就是我们在前面讲的 Agent 核心流程。

### 所谓费曼学习法，就是以教促学，用输出倒逼输入
