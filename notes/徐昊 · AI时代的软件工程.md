---
tags: [geek-time]
title: 徐昊 · AI时代的软件工程
created: '2025-03-17T13:32:21.684Z'
modified: '2025-03-17T13:32:56.214Z'
---

# 徐昊 · AI时代的软件工程

## 开篇词｜知识工程：AI时代的软件工程

虽然 ChatGPT 可以通过对话保持一定时间内的上下文信息，但是如果需要更稳定的反馈结果，可以修改原始的提示词，将上下文信息包含在其中，然后重新提问。这样会获得更稳定的反馈和结果

我们需要重复这样的过程，给予 ChatGPT 持续的反馈，明确我们需要修改的部分。在经过几次迭代之后，我们就会得到一个好得多的版本，当然也会有一个复杂得多的提示词

在与 ChatGPT 一起完成代码功能时，我们需要通过上下文信息补充，指导 ChatGPT 生成更接近我们所需的代码。这个过程中，具体的程序编写是由 ChatGPT 完成的，而要做什么、怎么做则是由人来指定的

软件是载体，并不是真正的产品，真正的产品是包含在其中的知识。软件只是知识的可执行形式。软件开发的核心不是产生代码，而是知识的获取与学习。 有效编码的前提是，所需的知识已经被有效地获取了。知识先存在于大脑中，然后才能体现在软件里。如果所有的知识都已经被掌握，那么编码也不会花费太多的气力。否则，开发过程就会迅速地转化为如何寻找并掌握正确的知识。

对于高水平的程序员而言，在敲下代码之前，80% 的代码都已经在头脑中构思好了。敲出来，也不过是一种必须的负担而已。所以写代码从来都不是软件开发的核心部分。 但编码的确是一个琐碎且繁杂的活动，它甚至掩盖了我们工作中最有价值的部分：获取、学习并传递知识

LLM 正在改变软件开发的面貌，它们可以自动化一些编码工作，但这并不意味着软件工程师的角色会消失。相反，它们将使软件工程师能够更多地专注于复杂问题的解决和创新，专注于更有价值的部分。也就是知识的获取与传递。 回想前面与 ChatGPT 一起写代码的例子，我们的主要工作，从编码转变成为 LLM 提供足够多的上下文信息。虽然我们并不需要提供详细的编码指导，也不需要指定类库或编程语言的用法，但是对于功能需求、业务知识、架构决策、测试策略等关乎“生产代码”的重要信息，还是要依赖我们，才能准确地提供给 LLM。 我们的关注点从如何构造软件，变成了如何提取组织知识，让知识变成 LLM 能够理解的形式。因此，提示词工程（Prompting Engineering）的关键并不在提示词，而在如何组织其中的知识。因而知识工程（Knowledge Engineering）是一个更合理的名字。

我们相当于从运动员变成了教练员。对于编码技能的要求降低了，但提高了对知识总结、知识传递、任务分解等技能的要求。正如传奇程序员 Kent Beck 所言，LLM 让我 90% 的技能无用了，却让 10% 的技能放大了 1000 倍。

对团队而言，大量的知识可以通过 LLM 得到有效的沉淀。原本困扰团队的诸多问题，比如，关键岗位人才流失造成大量上下文损失；在团队中难以形成统一的共识；新人培养时间长，难以上手等问题，都会得到极大的缓解。然而直接套用 LLM 并不能有效地实现这一点，需要我们有能力将现有研发流程重新梳理为知识管理的过程，从中捕获关键知识，并通过 LLM 加以沉淀。

在软件开发的不同阶段中，提取核心知识及关键载体，将它们表示成易于 LLM 理解的形式。其中编码阶段的知识提取和传递，将是个人与 LLM 合作的核心技能。

任何改变，只有彻底改变了团队中的人，才算是真正的落地，AI 也不例外

## 01｜围绕不可言说知识构造知识过程

软件工程也会逐渐变成知识工程（Knowledge Engineering），即提取知识、组织知识为 LLM 易于理解的形式，再通过 LLM 将这些知识转化为可工作的软件

知识工程的关键是提升知识传递效率

实际上在我们的工作中存在两类知识：显式知识（explicit knowledge）、不可言说的知识（tacit knowledge）。

不可言说的知识是指那些不易用言语表达或形式化的知识，它通常是个人经验、直觉或技能的一部分，与个人的认知和学习过程紧密相关。 不可言说的知识需要从经验中获取，很难通过语言或其他的形式传播。

显式知识是关于事实或规律的 know-what，不可言说知识是特定场景下的 know-how。

这其中的差别有点像求解计算题还是应用题。计算题只要根据计算法则，完成演算即可。而应用题有个额外的难点——我们先要从文字描述的场景里提取隐含的数学关系，然后才能选择正确的公式或算法求解问题。提取隐含的数学关系这个过程，就是不可言说知识在发挥作用。

不可言说知识不同于隐式知识（Implicit Knowledge），虽然在很多场合中我们也会使用隐式知识指代不可言说知识，但是二者还是存在很大的差别。隐式知识只是一种尚未被记录下来的显式知识，是没有记录下来的事实。要将隐式知识转化为显式知识，唯一需要做的步骤就是记录它。

在许多专业领域，如医学、工程、软件开发和艺术，高级技能往往是通过实践和模仿，而非仅通过书本学习获得。这些技能的核心是不可言说知识，它们对于专业实践至关重要。

不可言说知识的传递 当代知识管理理论认为，社会化活动（Socialization）是传递不可言说知识的不二法门。

不可言说知识通过共同活动进行交流，而不是书面或口头指令。比如在同一工作环境中，通过个体之间的互动完成不可言说知识的交换。学徒与导师一起工作，通过观察、模仿和实践来学习手工艺。在职培训也采用同样的原则。获取不可言说知识的关键是分享思维的过程，而不单单是消费最终的结果。

如果在轮扁询问学徒拿到材料和制式后的制作思路，并根据自己的经验提供反馈，那么借由思维过程的分享，最终应该还是能够教会学徒的。

所谓知识过程，就是从知识管理的角度理解我们的工作，将我们的工作看作产生、传递、应用、消费知识的过程。由于不可言说知识的重要性，将任何工作转化为知识过程的关键，就是识别其中关键的不可言说知识，围绕不可言说知识的传递构造流程。

显式知识是能够直接表达且在人群中分享的知识；隐式知识只是一种尚未被记录下来的显式知识，是没有记录下来的事实；不可言说知识，需要从经验中获取，很难通过语言或其他的形式传播。

## 02｜知识过程中的认知模式

了解到不可言说知识需要从经验中获取，很难通过语言或其他形式传播。想要把任何工作转化为知识工程，都需要识别其中关键的不可言说知识，并构造出知识传递效率更高的流程。

关键就是知识传递和消费的效率。而知识在传递和消费的过程中，会在知识的消费者身上产生不同的认知模式。通过认知行为模式，我们可以进一步理解知识传递和消费的效率。

2020 版 Cynefin 框架的定义，这五个领域分别是：清晰（Clear）、庞杂（Complicated）、复杂（Complex）、混乱（Chaotic）和困惑（Aporetic/Confused）。

这五种行为模式中，困惑（Aporectic/Confused）与其他四个不同，表示我们无法理解要解决的问题，因而当处在该模式时我们不能进行任何有效处理，是最低的认知水平。除困惑以外，处在其他四种认知模式时，我们都需要对问题进行诊断，并采取相应的行动，只是诊断与解决问题的方式不同而已。

对于问题的感知，通常是不可言说知识。定义问题永远比解决问题困难。

当处在复杂模式时，对于要解决的问题，并不知晓是否能够解决。无论前期做多少准备，都不能确保存在解决方案，而是需要依赖反思改进。

复杂模式的认知负载远远高于前面介绍的两种模式。首要原因在于其中问题的本质和解决方案都不是一开始就清晰可见的。在这种模式下，我们不能仅依赖于预设的规则或经验，而是必须不断地探索和适应，这本身就是一个学习过程。

## 03｜通过知识过程重新理解软件工程

将我们的工作看作产生、传递、应用、消费知识的过程

不可言说的知识是复杂工作中真正发挥作用的知识

软件本身是载体，并不是真正的产品，真正的产品是包含在其中的知识。 在这一点上，软件与书籍是一样的。虽然站在读者的角度看，书籍是产品，但作家真正关心的并不是书，而是其中的人物、情节、故事线。对于非虚构书籍而言，可能是观点、结构、论据等。真正有价值的并不是书，而是书中所蕴含的内容。所以作家可能不会过多地考虑书是如何被印刷出来的，但是要对如何组织其中的内容深思熟虑。对于作家而言，内容才是真正的产品，书籍只是内容的载体。 对于软件，真正的产品是软件中所包含的知识，软件自身仅仅是知识的载体。

作为产品的业务知识可以脱离软件存在，但是作为载体的软件却不能脱离其内在的知识存在

当然软件中还会承载其他的知识，比如软件的运营知识等等，但是从构造软件的角度上讲，最重要的两类知识就是业务知识和架构知识。一个是关于什么是“正确的软件”的知识，另一个则是关于“如何正确地构造软件”的知识。

由于业务知识并不仅仅存在于软件中，人和组织中也会存在业务知识。当引入软件承担一部分业务知识时，或是修改软件所承担的业务知识范围时，也会引起对应的组织变更。

我们工作中常用的方法，可能是低效的，而看起来奇怪的方法可能是高效的。 比如，Debug 实际是低效的复杂认知行为模式，探测（打断点）- 感知（通过断点周围的数据和调用栈，寻找问题成因）- 响应（定位问题）。 Debug 实际表示我们并不知道代码到底是如何运转的，正在学习当前代码库。而看起来奇怪的测试驱动开发（Test Driven Development）则是庞杂认知行为模式。

## 04｜使用LLM提取和传递知识

软件工程效率提升的问题，自然就转化成为构造知识传递效率更高的知识过程。

首先需要将 LLM 看作一种特殊的迁移学习（Transfer Learning）平台。

迁移学习（Transfer Learning）是机器学习领域的一个重要概念，意思是将从一个问题（源任务）中学到的知识应用到另一个相关的问题（目标任务）上。即使两个任务不完全相同，一个任务中学到的特征、模式和知识也可以在另一个任务中发挥作用，从而提高学习效率和性能。

LLM 是一个非常特殊的模型，它的核心能力是阅读理解——结合预训练集中的语料，根据提示词中提供的上下文信息执行任务。从概念上说，当我们为 LLM 提供了上下文之后，实际已经把 LLM 迁移成为一个面向特定领域的模型了。

我建议所有人在使用提示词的时候，都将知识的部分（背景）与任务的部分分开。这样能有一个明确的关注点

这种通过 LLM 的反馈反思并修改知识描述的方式，就是知识工程中的知识提取流程

通过 LLM 的辅助，利用知识与任务的分离，我们有效地收集和传递了显式化的隐式知识。但是对于知识过程中的核心——不可言说知识要怎么处理呢？

## 05｜使用LLM应用和提取不可言说知识

不可言说知识的应用会产生清晰（Clear）和庞杂（Complicated）认知模式，而不可言说知识的学习会产生复杂（Complex）认知模式。不同的认知行为模式具有不同的认知行为，因而与 LLM 交互的模式也就不尽相同。不同的交互模式，是我们处理不可言说知识应用和学习的关键。

虽然 ChatGPT 本身不支持链，但我们通过 LangChain 的 LCEL 很容易就能完成对两个模板的串联，在其他类似的框架里也有相应的支持。

处在庞杂模式下的时候，我们并不知道具体的任务有哪些，分别是什么。这时候我们可以使用知识生成（Generated Knowledge）让 LLM 帮助我们分析要处理的问题。

处在庞杂模式下，最有用的知识生成就是生成任务列表。因为任务列表本来就是分析的结果，是思路的体现。对于任务列表达成一致，就是思路的对齐。而一旦我们明确了需要 LLM 进行的任务，那么就进入了清晰模式。 任务列表的生成可以通过思维链（Chain of Thought，CoT）的方式来实现。CoT 是一种鼓励 LLM 解释其推理过程的方法。具体做法是向模型陈述推理过程。然后，要求 LLM 也要遵循类似的推理过程。

我们与 LLM 交互模式是借助通过思维链（或类似的技术）指导知识生成，明确 LLM 将要进行的工作。当我们与 LLM 就将要进行的任务达成一致，就可以按照这个共识，让 LLM 负责生成最后的结果。

与庞杂模式不同，在处在复杂模式时，我们对于要解决的问题甚至没有完整的思路。因而，响应并不是这个模式下主要要考虑的问题，获取思路才是重点。

不可言说知识的提取就是提取 CoT 的过程

在 RAG 架构下，我们提出的问题并不会被直接处理，而是根据提问的问题，首先从知识库中检索相关的片段，再将对应片段综合，生成最终的 prompt，提供给 LLM 进行回答。它包含根据问题检索、依据结果增强以及增强之后生成三个部分，因此而得名。

在不可言说知识应用的过程中，我们可以通过提示词模版建模任务和流程，来应用显式知识和充分学习的不可言说知识；通过已经提取完成的思维链，指导知识生成、产生任务列表，以消费不可言说知识。 对于不可言说知识的提取，我们可以使用知识提取流程，围绕思维链进行反馈提取。 另外，我们还可以在 LLM 辅助下，学习不可言说知识。

## 06｜LLM如何辅助软件交付？

可以看到，在这个模板中 API 要求的部分与需求背景部分是正交的。也就是说，无论针对何种需求背景，都可以依照同样的 API 要求，进行任务的操作。那么，对于 API 要求这部分的知识提取，就是可复用的知识。

对于庞杂行为模式，情况要稍微复杂一点。我们在使用 LLM 辅助庞杂认知模式的时候，通常会觉得 LLM 带来的效率提升很有限，完全不如自己直接处理来得快。

在庞杂模式下，自动化程度并不高。正如我们上节课所讲的，庞杂模式下首先要通过知识生成对齐思路。然后再根据思路（任务列表），指导 LLM 进行后续的任务。那么我们难以直接看到最终结果，因而感觉效率不高。

五种认知行为模式是不同的行为模式，而不是对于问题的划分。借助 Cynefin 框架，主要帮助我们理解对于同样的问题，人们有什么样的响应方式，而不是对问题本身的划分。

对于团队而言，效率的根源在于知识传递的效率，即知识传递的准确性，一致性和及时性，这些极大地影响着团队的效率。

通常我们认为复杂认知模式是一种低效的认知模式。因而在传统的管理方法中，我们会尽量地控制复杂认知模式的使用，转而追求以流程为核心，处于清晰认知模式的官僚机制，或是以专家为核心、处于庞杂认知模式的技术官僚机制。

LLM 的帮助下，对于某些场景而言，复杂认知模式的效率变得可以接受了。也就是只使用复杂认知模式而无需关注认知提升，依然可以高效地解决问题。

比如处理庞杂问题时，除了 CoT 之外，像推理行动（Reasoning-Action，ReAct）也可以处理已经充分学习的不可言说知识。而除了 RAG 以外，自动推理与工具使用（Automatic Reasoning and Tool-use）也是学习新知识的标准模式之一了（ChatGPT plugin 的基本架构）。

## 07｜通过业务建模应用业务知识

业务模型是如何帮助我们理解业务的。

软件开发的核心难度在于处理隐藏在业务知识中的复杂度，模型就是对这种复杂度的简化与精炼。

当得到了模型之后，怎么使用模型，帮助我们应用凝结在模型中的业务知识。这个过程，我称之为模型展开

通常我们会更关注于建模，也就是模型的提取过程，而往往忽略了模型提取后如何有效应用模型。

## 09｜LLM辅助建模（一）：构造反馈循环

模型是对于现实世界的抽象，是没有对错之分的，只有不同的角度和抽象的方式。我们评价一个模型，首先看能否适用于业务场景，然后是能否应对业务场景的变化。那么我们可以通过模型展开验证模型的适用度，以及应对变化的能力。

## 10｜LLM辅助建模（二）：构造思维链

Peter Coad 的四色法（4 Colors Modeling）

当然我们也可以使用其他的建模方法，比如催化剂方法（Catalysis）、实体目标法、事件风暴法（Event Storming）等等。 关键在于寻找到合适的中间产物，利用知识生成让 LLM 辅助提取核心概念。

如果已经存在相对完善的用户故事，那么我们可以相对容易地利用 LLM 提取模型。这就为遗留系统提供了很大便利。

如果已经存在相对完善的用户故事，那么我们可以相对容易地利用 LLM 提取模型。这就为遗留系统提供了很大便利。

## 11｜如何有效编写用户故事？

对于之前尝试使用用户故事管理需求的同学，可能一直有这么个疑问，用户故事一共也就三两句话，怎么能把复杂的功能需求说清楚呢？而这恰恰是用户故事的强大之处，也是用户故事能够匹配 LLM 的原因

这也就是用户故事与功能需求的一个重大差异。用户故事侧重于定义问题，而功能需求往往包含着解决方案。

问题的定义是困难的，而获取解决方案则容易得多。

有太多的文章和书籍是关于如何编写用户故事的，你可以参考 Mike Cohn 的《User Stories Appiled》以及 Alistair Cockburn《Writing Effective Use Cases》。

很显然，在一些场景下，用户登录对于用户本身并没有受益，比如，需要登录才能访问别人写的文章。这就是额外的步骤，并没有额外的价值，那么价值就不可能是发起操作的这个角色。类似的情况还有很多，比如员工打卡，显然员工自身的收益不会多过管理者，因而不能从员工的角度书写这个故事。

用户故事只有三种写法： So that 可以满足某个用户角色的目标 So that 可以满足整体解决方案的规则或流程 So that 可以进行用户旅程的下一步

用户故事的特性被总结为 Card、Conversation 和 Confirmation，这表示少而精的文字描述（Card），一段对话以及需与交付团队和客户澄清的细节。这是一个典型的不可言说知识。然而也从侧面表明了，用户故事本身就是一种不可言说知识的管理工具。 在 LLM 的时代，我们应该永远优先聚焦于问题的定义，然后才是解决方案。用户故事恰好符合这些条件，所以它是适用于 LLM 的需求表示形式。

## 13｜构建基于TQA模式的AI Agent

Thought-Question-Answer

ReAct 也是目前最热门的一个 LLM 模式。前一阵子（2023 年底）大火的 ChatGPT Plugin，还有 Microsoft 开源的 Semantic Kernel 都是基于 ReAct 模式。

ReAct 模式也成为了 LLM 与其他工具交互的主要模式。这种由 LLM 推理并控制其他工具调用的方式，也被称作 AI Agent。现在（2024 年）主流的 LLM 框架几乎都支持构建以 ReAct 为核心的 AI Agent。

需要注意一点，目前与推理有关的提示词受基础语料的影响极大，因而不同语言编写的提示词推理效果有显著差异。对于 ChatGPT 而言，ReAct 形式的提示词，用英语编写会有更好的结果。

可以看到 ChatGPT 进入了自问自答模式，并没有给人机会来提供真正的答案。想要实现人与 LLM 的互动问答，需要使用 LLM API 中的停止序列（Stop Sequence）。在模型生成内容的过程中，如果碰到指定的停止序列，模型就会停在那个位置。例如句子或列表的结尾，或者是问答环节中提问的部分。 显而易见，在 TQA 中，停止序列是 “Answer:”，也就是碰到需要等待用户给予反馈的时候停下，等用户输入完成之后，再继续后续的推理和任务。 对于不同的 LLM API 而言，停止序列的设定各有不同，请详细阅读文档。然后在前面给出的 Agent 例子中，替换提示词模板，就可以实现一个基于 TQA 的 Agent 了。

## 14｜业务知识管理中的LLM应用模式

从迭代软件交付的全流程上看，整个过程处在复杂的认知行为模式（Complex）。每一次迭代的过程，就是探测（Probe）的过程，收集反馈是感知（Sense）的行为，而最后对于目标和计划的调整就是响应（Respond）。

在互联网上有很多关于贪吃蛇的规则描述、示例代码等知识沉淀。也就是关于贪吃蛇的业务知识早就存在于互联网之上了，我们无需额外准备这些知识了。这里效率提升的主因并不是 LLM，而是包含在训练语料中的知识。因而我们无法跳过知识管理这一步，让 LLM 直接帮助我们生成我们想要的代码。

LLM 给绝大多数人带来的美好愿景。通过简单几句话的描述，LLM 就能帮助我们完成软件的开发。比如，在 ChatGPT 上线之初，我们看到过很多这样的例子： 请使用 JavaScript 编写一个贪吃蛇的游戏 然后 ChatGPT 就能够生成差不多的代码：

目前的 LLM 系统还存在技术限制。无论是 ChatGPT，Claude 或是别的 LLM 都存在上下文的限制，也就是每一次 LLM 只能处理有限数量的 token，以及产生有限数量 token 的结果。因而 LLM 能够理解的上下文规模，以及能够生成的应用规模都是有限的。对于大型系统而言，我们无法一次性将上下文传递给 LLM，也无法从 LLM 中一次性获取整个应用。也就是说 LLM 存在一个极限的认知负载。

而就算我们准备好了全部的知识，且我们的应用规模很小，就像贪吃蛇一样，规则明确，功能简单，代码量也不多。那么这时候，反馈循环的瓶颈就变成了我们如何验证 LLM 生成的系统是正确且有效的。要时刻记得，LLM 存在“幻觉”，会编造答案。 LLM 生成的回答是基于其在训练数据中学到的模式和信息。模型的目标是根据输入提供有用和合理的文本，但并不能真正理解问题的含义。有时候，模型可能会生成看似合理但事实上不正确或不准确的答案。 这将意味着，每一次 LLM 根据我们的反馈提供新的答案时，我们都需要进行全量回归测试（Full regression test）。哪怕对于贪吃蛇如此简单的系统，我们也需要测试操作、贪吃蛇变长、贪吃蛇碰到自身导致游戏终止，以及各种边界条件。这对于人来说，是极大的认知负载。

## 15｜使用LLM生成代码和测试

LLM 快则快矣，质量堪忧。当我们使用 LLM 辅助软件开发的时候，更多的精力要放到质量的控制上。而不是一味地关注效率。

## 16｜任务划分与测试驱动AI开发

而且在与 LLM 一起开发的时候，我们的关注点更多集中在测试上，通过测试提炼需要给予 LLM 的反馈，而不是编码。

LLM 存在技术限制，每一次 LLM 只能处理有限数量的 token 以及产生有限数量 token 的结果。因而 LLM 能够理解的上下文规模，以及能够生成的应用规模都是有限的。对于大型系统，我们无法一次性将上下文传递给 LLM，也无法从 LLM 中一次性获取整个应用。 那么使用 LLM 辅助软件交付的关键就在于将需求分解成足够小的任务，然后将这个任务转化为 LLM 的提示词，交由 LLM 处理，最后我们再将 LLM 的生成结果组合成生产或测试代码。

那么如何把任务划分成 LLM 易于处理的形式，就成为了使用 LLM 辅助软件开发的关键。而对于任务的划分，通常需要考虑两个维度，即软件的架构与测试策略。

任务总是以架构为基准划分的，架构最大的作用，也是指导每日工作的任务划分。当任务划分与架构规划不一致时，架构就会开始腐化。

反馈循环的瓶颈是我们如何验证 LLM 生成的系统是正确且有效的。比如上一节课，我们与 LLM 交互的过程中，大量的时间都花费在了验证和测试上。

## 17｜如何与LLM结对编程？

请根据上面的功能需求，列出需要测试的场景。描述场景，并给出相关的测试数据。

关键差别：以自然语言产生的测试 / 任务列表，我们更容易发现错误，并提出反馈。而以代码形式表示的功能代码，我们却很难在第一时间发现错误。

我们做出的改变有这么几个： 通过测试列表，更加关注与 LLM 对齐对于知识的理解； 以测试驱动的方式，遵守“红 - 绿 - 重构”的节奏； 按照“导航员 - 司机”的模式与 LLM 结对。 这些改变让我们在获得速度提升的时候，保证了代码的质量，得到了真正的效率提升

## 18｜测试策略（一）：如何构造有效的测试策略？

也就是能够提供快速反馈的细粒度测试占据多数，而缓慢昂贵的粗粒度测试应该只有一小部分。

其实这里非常容易忽略的一点是：每当有一个处于金字塔上层的测试失败，必然有多个处于底层的测试失败。

这才是测试金字塔的核心隐喻。下层的测试撑起上层的测试，而不是毫无关联的两套测试。

所以测试金字塔并不能帮助我们设计有效的测试策略，它只能帮我们检查我们的自动化测试集合是否处于良好状态，以及不同层之间的测试是否存在必要的关联。

## 20｜使用 LLM 按照测试策略生成代码

没有直接要求 LLM 帮助我们生成代码，还是与之前一样，先让 LLM 给出场景和测试数据

## 21｜什么是测试工序？

工序是指完成特定任务或生产产品所需的一系列步骤或程序。在制造业、生产领域或项目管理中，工序通常用于描述完成特定工作的方法或步骤。每个工序都有其独特的目标、方法和所需的资源。例如，在制造产品时，工序可以包括原材料的采购、加工、装配和质量控制等步骤。在项目管理中，工序则可以涵盖项目的规划、执行、监控和收尾等阶段。工序的定义和执行对于确保工作的有效进行和产品的质量至关重要。 而对于软件开发，工序由测试策略定义。

为什么我们不直接从架构上去设计工序，而要依赖于测试策略？这是因为，对于今日的软件开发，可测试性是进程内架构最重要的属性

## 23｜团队开发的核心模式

软件开发过程中的两个核心知识过程，即技术方案的应用以及软件质量的保证

我们要将这种不可言说知识形成团队共识（团队放大）。否则，团队中就会出现认知分歧。所谓认知分歧，是指一部分人处在庞杂认知行为模式，另一部分人处于复杂的认知模式。那么处于低认知行为的团队成员，就会引入质量缺陷，带来效率的损失

大把的架构师将精力放在技术方案的提前提取和准备上，而忽略了在团队中认知行为基线的建立

正如 Peter Drucker 所说，知识工作者的效率是知识消费的效率，而非知识生产的效率。

软件的质量有一点较为特殊，那就是它是适用性质量，而非符合性质量。

比如，一件工艺优秀但是不合身的衣服，就是适用性质量不足而符合性质量满分的产品。

软件的质量并不是由后验式的质量检查决定的，而是由软件开发的过程决定的。在软件开发的过程中，需要为软件的持续演化提供基础的支撑，这也决定了软件的质量。 这就是 Edwards Deming 的质量管理理论。他认为，优秀的结果不是偶然发生的，而是通过管理和改进过程来实现的。也就是著名的内建质量（Build Quality In）

## 24｜构造基于语义的自动化脚本

当我们与 LLM 交互时，有一个奇妙的心理预期，当我们处于复杂的认知模式时（Complex），我们对于 LLM 返回的结果是最满意的。因为我们对于要解决的问题十分懵懂，LLM 返回的结果只要对我们稍有启迪，我们就会非常满意。 比如，对于一个完全不了解的技术栈，哪怕是非常简单的功能，当 LLM 返回的代码可以达成一定功能时，我们就会惊讶于 LLM 的能力。其实这倒不是 LLM 有多么厉害，而是我们自己太菜了而已。我们菜到甚至无法正确判断 LLM 产生结果的质量。 而当我们处在清晰认知模式（Clear）时，我们又对 LLM 产生的结果异常地苛刻。因为此时我们对于要解决的问题，以及问题的解决方案有准确的认知，我们不光知道要解决的问题是什么，而且对于如何解决也有明确的期待。此时，我们就可以一眼看出 LLM 生成结果的成色。那么自然，对于 LLM 的结果也就没有那么满意了。 所以我们可以听到很多关于 LLM 截然相反的讨论，有人因为它给予我们启迪和思考，给予高度的赞扬，也有尝试将 LLM 用于实际工作屡屡碰壁，最后决定这东西没有什么太大用途。当然，我们现在知道，这是因为不同的认知行为模式造成的，在不同的认知行为模式下，LLM 能够发挥的作用也不尽相同。

## 25｜工具与框架

在大语言模型时代（Large Language Model，LLM），框架与工具将何去何从？

在大语言模型出现之前，我们复用知识的主要形式就是框架和工具。无论你是使用 ReactJS，还是 Spring Boot 编写前后端应用，实际上你复用了凝聚在这些框架中的知识。

在过去的三十年里，我们非常强调可工作的软件（Working Software），敏捷方法将可工作的软件作为度量进度的唯一标准。这是因为我们需要一个可执行的载体，来验证知识传递的效果。而今天，在大模型的帮助下，提炼出的知识几乎可以直接工作。我们是否还需要强调可工作的软件？还是应该直接关注可工作的知识？ 这并不是一个容易回答的问题。简单来说就是太早了，目前仍难有定论。

## 26｜知识过程下的团队管理

站在团队视角，我们会面临一个非常重要的问题，那就是认知分歧（Cognitive Diffusion）。也就是对于同一个问题，团队中不同的成员，可能会处在完全不同的认知模式。 比如，对于如何在项目中完成用户故事指定的功能，在项目上完全胜任的成员可能处在庞杂的认知模式下（Complicated），他们完成任务的行为是：感知（理解用户故事的上下文，以及验收条件）- 分析（按照测试工序指引，分解任务）- 响应（依据任务列表逐步完成工作）。 项目上不完全胜任的成员可能处于复杂的认知模式下（Complex），他们完成任务的行为是：探测（尝试 spike 一下故事卡中某些不清楚的地方）- 感知（按照得到的结果，重新理解要如何完成整张卡片）- 响应（逐步试错，完成功能）。 而刚毕业没有什么编程经验的成员，可能完全就是混乱模式了。他们甚至不会仔细辨别到底要做什么功能，就被巨大的恐慌驱动，冲过去写代码了。那么带来的自然是大量的返工和修改。

当我们处在无序认知的时候，不要相信自己的判断，理想的做法就是及时止损。

## 27｜围绕测试工序的认知对齐

对于一个熟练使用测试工序的团队而言，差不多两三个小时，可以完成整个迭代的任务拆分。在这个过程中，也可以重点照顾新加入的成员，让他们主导拆分的过程。团队中熟练的成员给予指导和建议即可。

另一个可行的方式是在代码审查（Code Review）中增加任务审查（Task Review）环节。与代码审查类似，任务审查也是一组人一起，对彼此的工作进行互评。所不同的是，后者关注点并不在最后的代码，而是拆分的任务。

说句题外话，代码审查在我看来是最没用的实践之一。当出现坏味道的代码时，重点并不是改掉代码，而是要理解为什么会写出这样的代码。而答案通常都是对于需求、架构或测试策略的误解。不消除这些误解，不好的代码还会源源不断地出现。 按精益理论来讲，就是比起修复缺陷重要的是修复人。这恰恰是代码审查时容易被忽略的部分，引入任务检查就好了很多，让我们更多地关注在思路的对齐，而非简单的结果校验。

## 28&结束语｜通过LLM构建团队门户

那么能不能利用多 Agents 架构，把范围进一步扩展到软件开发的全流程呢？ 简单来说，还不能。而且我也不太看好这个方向。想要理解其中的原因，还是要回到认知行为模式上。 当我们和 LLM 一起完成任务时，基本上是 LLM 负责干活人负责喊停。那么何时喊停，取决于人对于 LLM 产生内容质量的判断。也就是说，人对于问题的认知，和对于质量的理解，决定了最终人与 LLM 合作结果的质量与效率。

LLM 并不能直接提升认知水平。人的认知水平在某一时刻是一个客观的水准，并不随使用的工具而改变。


